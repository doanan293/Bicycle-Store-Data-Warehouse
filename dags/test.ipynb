{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query from /home/imdatsing/airflow/dags/__pycache__/BigQuery/Fact_RFM.sql executed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"/home/imdatsing/airflow/dags/__pycache__/seventh-jet-424513-h5-a426c383f9d7.json\"\n",
    "\n",
    "def read_sql_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to execute a SQL query\n",
    "def execute_query(file_path):\n",
    "    client = bigquery.Client(location=\"asia-southeast1\")\n",
    "    sql_query = read_sql_file(file_path)\n",
    "    query_job = client.query(sql_query)\n",
    "    query_job.result()\n",
    "    print(f\"Query from {file_path} executed successfully.\")\n",
    "    \n",
    "execute_query('/home/imdatsing/airflow/dags/__pycache__/BigQuery/Fact_RFM.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ALTER TABLE `seventh-jet-424513-h5.OLAP.Fact`\\nADD COLUMN IF NOT EXISTS recency_score INT64,\\nADD COLUMN IF NOT EXISTS frequency_score INT64,\\nADD COLUMN IF NOT EXISTS monetary_score INT64,\\nADD COLUMN IF NOT EXISTS RFM_segment STRING;\\n\\nCREATE OR REPLACE TABLE `seventh-jet-424513-h5.OLAP.LastOrderDate` AS\\nSELECT\\n  MAX(orderdate) AS last_order_date\\nFROM\\n  `seventh-jet-424513-h5.OLAP.Fact`;\\n\\nCREATE OR REPLACE TABLE `seventh-jet-424513-h5.OLAP.Fact_RFM` AS\\nWITH last_date AS (\\n  SELECT last_order_date FROM `seventh-jet-424513-h5.OLAP.LastOrderDate`\\n),\\nrfm_values AS (\\n  SELECT\\n    customerid,\\n    MAX(orderdate) AS last_order_date,\\n    COUNT(customerid) AS frequency,\\n    SUM(totalordertoUSD) AS monetary\\n  FROM\\n    `seventh-jet-424513-h5.OLAP.Fact`\\n  GROUP BY\\n    customerid\\n),\\nrfm_scores AS (\\n  SELECT\\n    customerid,\\n    DATE_DIFF((SELECT last_order_date FROM last_date), last_order_date, DAY) AS recency,\\n    frequency,\\n    monetary\\n  FROM\\n    rfm_values\\n),\\nrfm_with_scores AS (\\n  SELECT\\n    customerid,\\n    recency,\\n    frequency,\\n    monetary,\\n    NTILE(4) OVER (ORDER BY recency) AS recency_score,\\n    NTILE(4) OVER (ORDER BY frequency DESC) AS frequency_score,\\n    NTILE(4) OVER (ORDER BY monetary DESC) AS monetary_score\\n  FROM\\n    rfm_scores\\n),\\nrfm_segmented AS (\\n  SELECT\\n    customerid,\\n    recency_score,\\n    frequency_score,\\n    monetary_score,\\n    CONCAT(CAST(recency_score AS STRING), CAST(frequency_score AS STRING), CAST(monetary_score AS STRING)) AS RFM_score,\\n    CASE\\n      WHEN CONCAT(CAST(recency_score AS STRING), CAST(frequency_score AS STRING), CAST(monetary_score AS STRING)) = '444' THEN 'Best Customer'\\n      WHEN CONCAT(CAST(recency_score AS STRING), CAST(frequency_score AS STRING), CAST(monetary_score AS STRING)) = '111' THEN 'Churn Customer'\\n      WHEN RIGHT(CONCAT(CAST(recency_score AS STRING), CAST(frequency_score AS STRING), CAST(monetary_score AS STRING)), 1) = '4' THEN 'Highest Paying Customers'\\n      WHEN SUBSTR(CONCAT(CAST(recency_score AS STRING), CAST(frequency_score AS STRING), CAST(monetary_score AS STRING)), 2, 1) = '4' THEN 'Loyal Customer'\\n      WHEN LEFT(CONCAT(CAST(recency_score AS STRING), CAST(frequency_score AS STRING), CAST(monetary_score AS STRING)), 2) = '41' THEN 'Newest Customers'\\n      WHEN LEFT(CONCAT(CAST(recency_score AS STRING), CAST(frequency_score AS STRING), CAST(monetary_score AS STRING)), 2) = '11' THEN 'Once Loyal, Now Gone'\\n      ELSE 'Normal'\\n    END AS RFM_segment\\n  FROM\\n    rfm_with_scores\\n)\\nSELECT\\n  customerid,\\n  recency_score,\\n  frequency_score,\\n  monetary_score,\\n  RFM_segment\\nFROM\\n  rfm_segmented;\\n\\nMERGE `seventh-jet-424513-h5.OLAP.Fact` T\\nUSING `seventh-jet-424513-h5.OLAP.Fact_RFM` S\\nON T.customerid = S.customerid\\nWHEN MATCHED THEN\\n  UPDATE SET\\n    T.recency_score = S.recency_score,\\n    T.frequency_score = S.frequency_score,\\n    T.monetary_score = S.monetary_score,\\n    T.RFM_segment = S.RFM_segment;\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_sql_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "read_sql_file('/home/imdatsing/airflow/dags/__pycache__/BigQuery/Fact_RFM.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.4008638617217284e-22\n",
      "Results successfully loaded to BigQuery table seventh-jet-424513-h5.Machine_Learning_Results.LinearRegressionResults\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_linear_regression_model():\n",
    "    # Initialize BigQuery client\n",
    "    client = bigquery.Client(location=\"asia-southeast1\")\n",
    "    \n",
    "    # Define the query to retrieve data from the Fact table\n",
    "    query = \"\"\"\n",
    "    SELECT * FROM `seventh-jet-424513-h5.OLAP.Fact`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query and convert the result to a Pandas DataFrame\n",
    "    df = client.query(query).to_dataframe()\n",
    "    \n",
    "    # Data preprocessing\n",
    "    df['orderdate'] = pd.to_datetime(df['orderdate'])  # Convert orderdate to datetime\n",
    "    df['order_year'] = df['orderdate'].dt.year         # Extract year from orderdate\n",
    "    df = df.drop(['orderdate', 'salesorderid'], axis=1)  # Drop orderdate and salesorderid columns\n",
    "    \n",
    "    # Fill missing values with 0\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # Select features (X) and target (y)\n",
    "    X = df.drop('totaldue', axis=1)  # Features: all columns except totaldue\n",
    "    y = df['totaldue']               # Target: totaldue\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model using Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    \n",
    "    # Create a DataFrame with the test set and predictions\n",
    "    results_df = X_test.copy()\n",
    "    results_df['actual_totaldue'] = y_test\n",
    "    results_df['predicted_totaldue'] = predictions\n",
    "    \n",
    "    # Evaluate the model using Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    \n",
    "    # Push the results to BigQuery\n",
    "    table_id = \"seventh-jet-424513-h5.Machine_Learning_Results.LinearRegressionResults\"\n",
    "    job = client.load_table_from_dataframe(results_df, table_id)\n",
    "    job.result()  # Wait for the job to complete\n",
    "    print(f\"Results successfully loaded to BigQuery table {table_id}\")\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering results successfully loaded to BigQuery table seventh-jet-424513-h5.Machine_Learning_Results.ClusteringResults\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set the Google Cloud credentials environment variable\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"/home/imdatsing/airflow/dags/__pycache__/seventh-jet-424513-h5-a426c383f9d7.json\"\n",
    "\n",
    "def customer_clustering():\n",
    "    # Initialize BigQuery client\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    # Define the query to retrieve customer data\n",
    "    query = \"\"\"\n",
    "    SELECT * FROM `your_dataset.your_customer_table`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query and convert the result to a Pandas DataFrame\n",
    "    df = client.query(query).to_dataframe()\n",
    "    \n",
    "    # Data preprocessing\n",
    "    # (Add your preprocessing steps here if needed)\n",
    "    \n",
    "    # Feature selection for clustering\n",
    "    features = df.drop(['customer_id'], axis=1)  # Exclude customer_id column from features\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    # Perform PCA to reduce dimensionality for better clustering visualization (optional)\n",
    "    pca = PCA(n_components=2)\n",
    "    features_pca = pca.fit_transform(features_scaled)\n",
    "    \n",
    "    # Perform K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42)  # Assume we want 5 clusters\n",
    "    df['cluster'] = kmeans.fit_predict(features_scaled)\n",
    "    \n",
    "    # Push the results to BigQuery\n",
    "    table_id = \"your_dataset.customer_clusters\"\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()  # Wait for the job to complete\n",
    "    print(f\"Customer clustering results successfully loaded to BigQuery table {table_id}\")\n",
    "\n",
    "train_clustering_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('HYT00', '[HYT00] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd          # pip install pandas\n",
    "import pypyodbc as odbc      # pip install pypyodbc\n",
    "from google.cloud import bigquery  # pip install google-cloud-bigquery\n",
    "import io                    # Import the io module\n",
    "import os\n",
    "from google.oauth2 import service_account\n",
    "from sqlserver import SQLServer\n",
    "\n",
    "class SQLServer:\n",
    "    DRIVER_NAME = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "    def __init__(self, server_name, database_name, username, password):\n",
    "        self.server_name = server_name\n",
    "        self.database_name = database_name\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.conn = None\n",
    "\n",
    "    def connect_to_sql_server(self):\n",
    "        try:\n",
    "            self.conn = odbc.connect(self._connection_string())\n",
    "            print('Connected')\n",
    "            return self.conn\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "\n",
    "    def _connection_string(self):\n",
    "        conn_string = f\"\"\"\n",
    "            DRIVER={{{self.DRIVER_NAME}}};\n",
    "            SERVER={self.server_name};\n",
    "            DATABASE={self.database_name};\n",
    "            UID={self.username};\n",
    "            PWD={self.password};\n",
    "        \"\"\"\n",
    "        return conn_string\n",
    "\n",
    "    def query(self, sql_statement):\n",
    "        if self.conn is None:\n",
    "            print('Connection is absent')\n",
    "            return None, None\n",
    "        try:\n",
    "            cursor = self.conn.cursor()\n",
    "            cursor.execute(sql_statement)\n",
    "            data = cursor.fetchall()\n",
    "            columns = [col[0] for col in cursor.description]\n",
    "            return columns, data\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None, None\n",
    "\n",
    "    def cursor(self):\n",
    "        if not self.check_connection():\n",
    "            print('Connection is absent')\n",
    "            return None\n",
    "        return self.conn.cursor()\n",
    "\n",
    "    def check_connection(self):\n",
    "        return self.conn is not None\n",
    "\n",
    "# Step 1. Connect to SQL Server database\n",
    "SERVER_NAME = '172.30.114.39'\n",
    "DATABASE_NAME = 'AdventureWorks2022'\n",
    "USERNAME = 'sa'\n",
    "PASSWORD = 'Imdatsing15122003.'\n",
    "\n",
    "# Create a connection string\n",
    "sql_server_instance = SQLServer(SERVER_NAME, DATABASE_NAME,USERNAME,PASSWORD)\n",
    "sql_server_instance.connect_to_sql_server()\n",
    "\n",
    "# Step 2. Set up Google Cloud credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"/home/imdatsing/airflow/dags/__pycache__/seventh-jet-424513-h5-a426c383f9d7.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_bigquery(sql_query, table_id):\n",
    "    columns, records = sql_server_instance.query(sql_query)\n",
    "    df = pd.DataFrame(records, columns=columns)\n",
    "    df[\"Created_dt\"] = datetime.now()\n",
    "\n",
    "    # Define table schema\n",
    "    client = bigquery.Client(location=\"asia-southeast1\")\n",
    "\n",
    "    # Configure job\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,\n",
    "        write_disposition='WRITE_TRUNCATE'\n",
    "    )\n",
    "\n",
    "    # Convert the DataFrame to a CSV string\n",
    "    csv_data = df.to_csv(index=False)\n",
    "\n",
    "    # Load data to BigQuery from CSV string\n",
    "    job = client.load_table_from_file(\n",
    "        file_obj=io.StringIO(csv_data),\n",
    "        destination=table_id,\n",
    "        job_config=job_config\n",
    "    )\n",
    "\n",
    "    # Wait for the load job to complete\n",
    "    job.result()\n",
    "    print(f\"Data loaded successfully to BigQuery table {table_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully to BigQuery table seventh-jet-424513-h5.STAGING.Customer\n",
      "('42000', \"[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]There is insufficient system memory in resource pool 'internal' to run this query.\")\n",
      "Data loaded successfully to BigQuery table seventh-jet-424513-h5.STAGING.CurrencyRate\n",
      "('HY000', '[HY000] [Microsoft][ODBC Driver 17 for SQL Server]Unspecified error occurred on SQL Server. Connection may have been terminated by the server.')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Cursor.__del__ at 0x7f021a58feb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/imdatsing/.local/lib/python3.10/site-packages/pypyodbc.py\", line 2389, in __del__\n",
      "    self.close()\n",
      "  File \"/home/imdatsing/.local/lib/python3.10/site-packages/pypyodbc.py\", line 2372, in close\n",
      "    check_success(self, ret)\n",
      "  File \"/home/imdatsing/.local/lib/python3.10/site-packages/pypyodbc.py\", line 1007, in check_success\n",
      "    ctrl_err(SQL_HANDLE_STMT, ODBC_obj.stmt_h, ret, ODBC_obj.ansi)\n",
      "  File \"/home/imdatsing/.local/lib/python3.10/site-packages/pypyodbc.py\", line 987, in ctrl_err\n",
      "    raise DatabaseError(state,err_text)\n",
      "pypyodbc.DatabaseError: ('08S01', '[08S01] [Microsoft][ODBC Driver 17 for SQL Server]Communication link failure')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully to BigQuery table seventh-jet-424513-h5.STAGING.Currency\n",
      "('08S01', '[08S01] [Microsoft][ODBC Driver 17 for SQL Server]TCP Provider: Error code 0x2746')\n",
      "Data loaded successfully to BigQuery table seventh-jet-424513-h5.STAGING.SalesOrderHeader\n",
      "('08S01', '[08S01] [Microsoft][ODBC Driver 17 for SQL Server]Communication link failure')\n",
      "Data loaded successfully to BigQuery table seventh-jet-424513-h5.STAGING.SalesOrderDetail\n",
      "('08S01', '[08S01] [Microsoft][ODBC Driver 17 for SQL Server]Communication link failure')\n",
      "Data loaded successfully to BigQuery table seventh-jet-424513-h5.STAGING.Product\n",
      "('08S01', '[08S01] [Microsoft][ODBC Driver 17 for SQL Server]Communication link failure')\n",
      "Data loaded successfully to BigQuery table seventh-jet-424513-h5.STAGING.ProductSubcategory\n",
      "('08S01', '[08S01] [Microsoft][ODBC Driver 17 for SQL Server]Communication link failure')\n",
      "Data loaded successfully to BigQuery table seventh-jet-424513-h5.STAGING.ProductCategory\n",
      "('08S01', '[08S01] [Microsoft][ODBC Driver 17 for SQL Server]Communication link failure')\n",
      "Data loaded successfully to BigQuery table seventh-jet-424513-h5.STAGING.StateProvince\n",
      "('08S01', '[08S01] [Microsoft][ODBC Driver 17 for SQL Server]Communication link failure')\n",
      "Data loaded successfully to BigQuery table seventh-jet-424513-h5.STAGING.CountryRegion\n",
      "('08S01', '[08S01] [Microsoft][ODBC Driver 17 for SQL Server]Communication link failure')\n",
      "Data loaded successfully to BigQuery table seventh-jet-424513-h5.STAGING.SalesOrderHeaderSalesReason\n",
      "('08S01', '[08S01] [Microsoft][ODBC Driver 17 for SQL Server]Communication link failure')\n",
      "Data loaded successfully to BigQuery table seventh-jet-424513-h5.STAGING.SalesReason\n"
     ]
    }
   ],
   "source": [
    "# Define the SQL queries and corresponding BigQuery table IDs\n",
    "tasks = [\n",
    "    {\"sql_query\": \"SELECT * FROM Sales.Customer\", \"table_id\": \"seventh-jet-424513-h5.STAGING.Customer\"},\n",
    "\n",
    "    {\"sql_query\": \"SELECT * FROM Sales.CurrencyRate\", \"table_id\": \"seventh-jet-424513-h5.STAGING.CurrencyRate\"},\n",
    "\n",
    "    {\"sql_query\": \"SELECT * FROM Sales.Currency\", \"table_id\": \"seventh-jet-424513-h5.STAGING.Currency\"},\n",
    "\n",
    "    {\"sql_query\": \"SELECT * FROM Sales.SalesOrderHeader\", \"table_id\": \"seventh-jet-424513-h5.STAGING.SalesOrderHeader\"},\n",
    "\n",
    "    {\"sql_query\": \"SELECT * FROM Sales.SalesOrderDetail\", \"table_id\": \"seventh-jet-424513-h5.STAGING.SalesOrderDetail\"},\n",
    "\n",
    "    {\"sql_query\": \"SELECT * FROM Production.Product\", \"table_id\": \"seventh-jet-424513-h5.STAGING.Product\"},\n",
    "\n",
    "    {\"sql_query\": \"SELECT * FROM Production.ProductSubcategory\", \"table_id\": \"seventh-jet-424513-h5.STAGING.ProductSubcategory\"},\n",
    "\n",
    "    {\"sql_query\": \"SELECT * FROM Production.ProductCategory\", \"table_id\": \"seventh-jet-424513-h5.STAGING.ProductCategory\"},\n",
    "\n",
    "    {\"sql_query\": \"SELECT * FROM Person.StateProvince\", \"table_id\": \"seventh-jet-424513-h5.STAGING.StateProvince\"},\n",
    "\n",
    "    {\"sql_query\": \"SELECT * FROM Person.CountryRegion\", \"table_id\": \"seventh-jet-424513-h5.STAGING.CountryRegion\"},\n",
    "\n",
    "    {\"sql_query\": \"SELECT * FROM Sales.SalesOrderHeaderSalesReason\", \"table_id\": \"seventh-jet-424513-h5.STAGING.SalesOrderHeaderSalesReason\"},\n",
    "\n",
    "    {\"sql_query\": \"SELECT * FROM Sales.SalesReason\", \"table_id\": \"seventh-jet-424513-h5.STAGING.SalesReason\"},\n",
    "]\n",
    "\n",
    "# Run the load process for each task\n",
    "for task in tasks:\n",
    "    load_data_to_bigquery(task[\"sql_query\"], task[\"table_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 PUT https://bigquery.googleapis.com/upload/bigquery/v2/projects/seventh-jet-424513-h5/jobs?uploadType=resumable&upload_id=ABPtcPqlqFOxz4kQu5zDLqI5FtCXikKAWWLj98TR0NSZhSpUHOtiz09zP9CgFjhiJ1zL0MktbXwG7x524S0uUyS3OC-k8VrasSjAdyqyXMAsC9ui: Invalid request.  There were 1624989 byte(s) (or more) in the request body.  There should have been 1623125 byte(s) (starting at offset 0 and ending at offset 1623124) according to the Content-Range header.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidResponse\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/bigquery/client.py:2580\u001b[0m, in \u001b[0;36mClient.load_table_from_file\u001b[0;34m(self, file_obj, destination, rewind, size, num_retries, job_id, job_id_prefix, location, project, job_config, timeout)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _MAX_MULTIPART_SIZE:\n\u001b[0;32m-> 2580\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_resumable_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_resource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\n\u001b[1;32m   2582\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/bigquery/client.py:3005\u001b[0m, in \u001b[0;36mClient._do_resumable_upload\u001b[0;34m(self, stream, metadata, num_retries, timeout, project)\u001b[0m\n\u001b[1;32m   3004\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m upload\u001b[38;5;241m.\u001b[39mfinished:\n\u001b[0;32m-> 3005\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mupload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransmit_next_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/resumable_media/requests/upload.py:515\u001b[0m, in \u001b[0;36mResumableUpload.transmit_next_chunk\u001b[0;34m(self, transport, timeout)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_request_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_and_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriable_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_status_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_strategy\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/resumable_media/requests/_request_helpers.py:155\u001b[0m, in \u001b[0;36mwait_and_retry\u001b[0;34m(func, get_status_code, retry_strategy)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _CONNECTION_ERROR_CLASSES \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/resumable_media/requests/upload.py:511\u001b[0m, in \u001b[0;36mResumableUpload.transmit_next_chunk.<locals>.retriable_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    507\u001b[0m result \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    508\u001b[0m     method, url, data\u001b[38;5;241m=\u001b[39mpayload, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    509\u001b[0m )\n\u001b[0;32m--> 511\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_resumable_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/resumable_media/_upload.py:690\u001b[0m, in \u001b[0;36mResumableUpload._process_resumable_response\u001b[0;34m(self, response, bytes_sent)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Process the response from an HTTP request.\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \n\u001b[1;32m    672\u001b[0m \u001b[38;5;124;03mThis is everything that must be done after a request that doesn't\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m.. _sans-I/O: https://sans-io.readthedocs.io/\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 690\u001b[0m status_code \u001b[38;5;241m=\u001b[39m \u001b[43m_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_status_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPERMANENT_REDIRECT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_status_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_invalid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status_code \u001b[38;5;241m==\u001b[39m http\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mOK:\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;66;03m# NOTE: We use the \"local\" information of ``bytes_sent`` to update\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;66;03m#       ``bytes_uploaded``, but do not verify this against other\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m#       * ``stream.tell()`` (relying on fact that ``initiate()``\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m#         requires stream to be at the beginning)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/resumable_media/_helpers.py:108\u001b[0m, in \u001b[0;36mrequire_status_code\u001b[0;34m(response, status_codes, get_status_code, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m         callback()\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m common\u001b[38;5;241m.\u001b[39mInvalidResponse(\n\u001b[1;32m    109\u001b[0m         response,\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest failed with status code\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    111\u001b[0m         status_code,\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected one of\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;241m*\u001b[39mstatus_codes\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status_code\n",
      "\u001b[0;31mInvalidResponse\u001b[0m: ('Request failed with status code', 400, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PERMANENT_REDIRECT: 308>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m csv_data \u001b[38;5;241m=\u001b[39m Address\u001b[38;5;241m.\u001b[39mto_csv(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Load data to BigQuery from CSV string\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m job \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_table_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStringIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdestination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_config\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Wait for the load job to complete\u001b[39;00m\n\u001b[1;32m     48\u001b[0m job\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/bigquery/client.py:2588\u001b[0m, in \u001b[0;36mClient.load_table_from_file\u001b[0;34m(self, file_obj, destination, rewind, size, num_retries, job_id, job_id_prefix, location, project, job_config, timeout)\u001b[0m\n\u001b[1;32m   2584\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_multipart_upload(\n\u001b[1;32m   2585\u001b[0m             file_obj, job_resource, size, num_retries, timeout, project\u001b[38;5;241m=\u001b[39mproject\n\u001b[1;32m   2586\u001b[0m         )\n\u001b[1;32m   2587\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m resumable_media\u001b[38;5;241m.\u001b[39mInvalidResponse \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m-> 2588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(exc\u001b[38;5;241m.\u001b[39mresponse)\n\u001b[1;32m   2590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typing\u001b[38;5;241m.\u001b[39mcast(LoadJob, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_from_resource(response\u001b[38;5;241m.\u001b[39mjson()))\n",
      "\u001b[0;31mBadRequest\u001b[0m: 400 PUT https://bigquery.googleapis.com/upload/bigquery/v2/projects/seventh-jet-424513-h5/jobs?uploadType=resumable&upload_id=ABPtcPqlqFOxz4kQu5zDLqI5FtCXikKAWWLj98TR0NSZhSpUHOtiz09zP9CgFjhiJ1zL0MktbXwG7x524S0uUyS3OC-k8VrasSjAdyqyXMAsC9ui: Invalid request.  There were 1624989 byte(s) (or more) in the request body.  There should have been 1623125 byte(s) (starting at offset 0 and ending at offset 1623124) according to the Content-Range header."
     ]
    }
   ],
   "source": [
    "# Step 2. Query the records from SQL Server\n",
    "sql_statement = \"\"\"\n",
    "SELECT \n",
    "AddressID,\n",
    "AddressLine1,\n",
    "AddressLine2,\n",
    "City,\n",
    "StateProvinceID\n",
    "\n",
    "FROM Person.Address \n",
    "\"\"\"\n",
    "\n",
    "columns, records = sql_server_instance.query(sql_statement)\n",
    "Address = pd.DataFrame(records, columns=columns)\n",
    "Address[\"Created_dt\"] = datetime.now()\n",
    "Address[\"addressline1\"] = Address[\"addressline1\"].astype(str)\n",
    "Address[\"addressline2\"] = Address[\"addressline1\"].astype(str)\n",
    "\n",
    "# Step 3. Define table schema\n",
    "client = bigquery.Client(location=\"asia-southeast1\")\n",
    "\n",
    "# If you don't want to specify column schema, set autodetect to True\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    autodetect=True,\n",
    "    #schema=[\n",
    "        #bigquery.SchemaField(\"orderdate\", 'TIMESTAMP'),\n",
    "        #bigquery.SchemaField(\"duedate\", 'TIMESTAMP'),\n",
    "        #bigquery.SchemaField(\"shipdate\", 'TIMESTAMP'),\n",
    "    #],\n",
    "    # WRITE_APPEND, WRITE_EMPTY\n",
    "    write_disposition='WRITE_TRUNCATE'\n",
    ")\n",
    "\n",
    "# Step 4. Load data to BigQuery\n",
    "table_id = 'seventh-jet-424513-h5.STAGING.Address'  # Replace with your project and dataset ID\n",
    "\n",
    "# Convert the DataFrame to a CSV string\n",
    "csv_data = Address.to_csv(index=False)\n",
    "\n",
    "# Load data to BigQuery from CSV string\n",
    "job = client.load_table_from_file(\n",
    "    file_obj=io.StringIO(csv_data),\n",
    "    destination=table_id,\n",
    "    job_config=job_config\n",
    ")\n",
    "\n",
    "# Wait for the load job to complete\n",
    "job.result()\n",
    "\n",
    "print(\"Data loaded successfully to BigQuery\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
